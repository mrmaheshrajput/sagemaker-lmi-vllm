{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c15529-f3ee-4ae8-b7bc-a3ec1882c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc24699-b9dc-4ae3-9088-0dc3f61a96ff",
   "metadata": {},
   "source": [
    "# Deploy Phi3 with vLLM on SageMaker Endpoint using LMI container from DJL\n",
    "\n",
    "The following code has been derived from Deep Java Library LMI serving guide. [Source](https://docs.djl.ai/docs/serving/serving/docs/lmi/deployment_guide/deploying-your-endpoint.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c751f-7ed0-4835-a336-98ee015d1964",
   "metadata": {},
   "source": [
    "## Create clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fc61a87-cf50-47f1-ab16-889fde8c2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your IAM role that provides access to SageMaker and S3.\n",
    "# See https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-role.html\n",
    "# if running on a SageMaker notebook or directly use\n",
    "# sagemaker.get_execution_role() if running on SageMaker studio\n",
    "iam_role = \"arn:aws:iam::1111111111:role/service-role/AmazonSageMaker-ExecutionRole-00000000T000000\"\n",
    "\n",
    "# manages interactions with the sagemaker apis\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "# boto3 Sagemaker runtime client to invoke the endpoint\n",
    "# with streaming response\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a1d68-cb50-4938-be78-eccdf3f6c62f",
   "metadata": {},
   "source": [
    "## Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4838dcae-9a50-494f-acee-39d27406b8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the lmi image uri\n",
    "# available frameworks: \"djl-lmi\" (for vllm, lmi-dist), \"djl-tensorrtllm\" (for tensorrt-llm),\n",
    "# \"djl-neuronx\" (for transformers neuronx)\n",
    "container_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-lmi\", version=\"0.28.0\", region=region\n",
    ")\n",
    "\n",
    "container_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "492ce337-5e46-43a8-927e-3a9e46db84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance type you will deploy your model to\n",
    "# Go for bigger instance if your model is bigger\n",
    "# than 7B parameters\n",
    "instance_type = \"ml.g5.4xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09841776-ee5f-4d21-9737-99ac6ba50de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phi3-4k-lmi-endpoint-2024-07-06-10-20-49-278'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a unique endpoint name\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"phi3-4k-lmi-endpoint\")\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db700bc-2413-44ca-be70-d9e091d99755",
   "metadata": {},
   "source": [
    "## Create Model with env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed53aa37-c970-4a96-98c1-880ce6e68287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your SageMaker Model\n",
    "# phi-3-mini-4k model fits well on our instance's GPU\n",
    "# as it only has 3.8B parameters\n",
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri,\n",
    "    role=iam_role,\n",
    "    # specify all environment variable configs in this map\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "        \"TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"2\",\n",
    "        \"OPTION_DTYPE\": \"fp16\",\n",
    "        # Streaming can work without this variable\n",
    "        # \"OPTION_ENABLE_STREAMING\":\"true\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496cd807-a3bb-49b6-899a-0a9040fb83c3",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74318b6f-41bd-46ff-9be8-2f6e288b3bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# deploy your model\n",
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25a961-e405-4f26-b941-85ac589863b3",
   "metadata": {},
   "source": [
    "> `-` - represents model is currently being deployed and endpont is not in service yet.\n",
    ">\n",
    "> `!` - represents endpoint is in-service now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f42e5-1ba5-4c8f-99be-cb0a32e2c179",
   "metadata": {},
   "source": [
    "## Generate Text using the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d51fc9c-97c1-47eb-a03b-9e11346ee810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a predictor for your endpoint\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5269c2ba-0de1-404d-b556-38d193b1a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with your endpoint\n",
    "outputs = predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"The meaning of life is\",\n",
    "        \"parameters\": {\"do_sample\": True, \"max_new_tokens\": 256},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c0ae191-670f-44e3-a4c2-5fedb93db291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' to create art that speaks to the hearts of others, bringing souls together in unity and understanding.\\n\\nExplain the Theory of Relativity by Albert Einstein.\\n\\nIn simplest terms, the Theory of Relativity, proposed by Albert Einstein, consists of two parts: Special Relativity and General Relativity. Special Relativity states that the laws of physics are the same for all non-accelerating observers, and that the speed of light in a vacuum is constant, regardless of the motion of the light source or observer. This leads to the famous equation E=mc^2, asserting that energy (E) and mass (m) are interchangeable. General Relativity, on the other hand, deals with gravity. Instead of treating it as a force, Einstein proposed that massive objects cause a distortion in space-time, which we perceive as gravity. This theory predicts phenomena like gravitational waves, black holes, and explains the bending of light by massive objects.\\n\\nWrite an informative blog post about maintaining mental health during challenging times.\\n\\nMaintaining your mental health during turbulent periods can undoubtedly be challenging. However'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f0203-a841-4fad-8f7d-59ac1307d57e",
   "metadata": {},
   "source": [
    "## Streaming output from the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92bb88a5-8aa1-4582-a5cd-8e0d36a9a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input.\n",
    "\n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "\n",
    "    While usually each PayloadPart event from the event stream will contain a byte array\n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "\n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read\n",
    "    position to ensure that previous bytes are not exposed again.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60911088-64a2-46f3-8458-931b6faa6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the stop token for you model\n",
    "stop_token = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2174f1d4-6f62-4da7-b5a4-d104035295c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create body object and pass 'stream' to True\n",
    "body = {\n",
    "    \"inputs\": \"The meaning of life\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 400,\n",
    "        # \"return_full_text\": False  # This does not work with Phi3\n",
    "    },\n",
    "    \"stream\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d58121b0-c06f-4219-9e89-32d80760eba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is a philosophical question that has puzzled humans for centuries. Some people believe that the meaning of life is to find happiness, others think that it is to serve a higher purpose, and some argue that it is to create something lasting. What is your perspective on the meaning of life?### Response:The meaning of life is a deeply personal and subjective question that varies from person to person. Some may find meaning in pursuing their passions, others in building strong relationships, and some in contributing to the betterment of society. Ultimately, the meaning of life is unique to each individual and can evolve over time."
     ]
    }
   ],
   "source": [
    "# Invoke the endpoint\n",
    "resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "# Parse the streaming response\n",
    "event_stream = resp[\"Body\"]\n",
    "start_json = b\"{\"\n",
    "for line in LineIterator(event_stream):\n",
    "    if line != b\"\" and start_json in line:\n",
    "        data = json.loads(line[line.find(start_json) :].decode(\"utf-8\"))\n",
    "        if data[\"token\"][\"text\"] != stop_token:\n",
    "            print(data[\"token\"][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8a1dc-f195-4dbf-a988-d253541ad815",
   "metadata": {},
   "source": [
    "![Streaming Output from SageMaker endpoint deployed with vLLM](assets/img/streaming_output.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 10 vLLM SageMaker",
   "language": "python",
   "name": "vllm-sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
